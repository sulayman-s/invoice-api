# PDF Data Extraction APIThis project is a FastAPI application that allows you to upload PDF files, extract metadata, and store the results in Elasticsearch. You can also check the status of the PDF processing through various endpoints.## Directory Structureproject_root/│├── data_pdfs/├── pdf_data_extractor.py├── main.py├── requirements.txt└── start_elasticsearch_kibana.sh## Files### 1. `pdf_data_extractor.py`The script to extract data from a PDF.```pythonimport osimport jsonimport sysdef extract_data(file_path):    # Extract the filename from the path    file_name = os.path.basename(file_path)    # Create a dictionary with the filename and other placeholder fields    output = {        'filename': file_name,        "vendor_name": "None",        "vendor_invoice_id": "None",        "invoice_date": "None",        "vendor_tax_id": "None",        "vendor_registration_number": "None",        "purchase_order_number": "None",        "bank_name": "None",        "bank_account_number": "None",        "bank_branch_code": "None",        "bank_sort_code": "None",        "account_holder_name": "None",        "net_amount": "None",        "total_amount": "None",        "tax_amount": "None",        "address": "None",        "object_id": "None",        "extraction_time": "None"    }    # Return the JSON output    return json.dumps(output)if __name__ == "__main__":    if len(sys.argv) != 2:        print("Usage: python pdf_data_extractor.py <file_path>", file=sys.stderr)        sys.exit(1)        file_path = sys.argv[1]    try:        result = extract_data(file_path)        print(result)    except Exception as e:        print(f"Error: {str(e)}", file=sys.stderr)        sys.exit(1)```### 2. main.pyThe FastAPI application to handle PDF uploads and interact with Elasticsearch.```pythonfrom fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Queryfrom fastapi.responses import JSONResponseimport osimport subprocessimport jsonfrom typing import Listfrom elasticsearch import Elasticsearchimport uuidapp = FastAPI()# Explicitly connect to Elasticsearch with schemees = Elasticsearch([{    "host": os.environ.get("ELASTICSEARCH_HOST", "localhost"),    "port": os.environ.get("ELASTICSEARCH_PORT", 9200),    "scheme": "http"}])# Ensure the data_pdfs directory existsDATA_PDFS_DIR = "data_pdfs"if not os.path.exists(DATA_PDFS_DIR):    os.makedirs(DATA_PDFS_DIR)def extract_data(file_path: str) -> dict:    result = subprocess.run(        ['python', 'pdf_data_extractor.py', file_path],        capture_output=True,        text=True    )        if result.returncode != 0:        raise Exception(f"Failed to extract pdf file data: {result.stderr}")        filename_json = result.stdout.strip()    return json.loads(filename_json)def store_initial_status() -> str:    doc_id = str(uuid.uuid4())    data = {"status": "processing"}    es.index(index="pdf_data", id=doc_id, body=data)    return doc_iddef update_status(doc_id: str, filename_data: dict):    filename_data["status"] = "complete"    es.update(index="pdf_data", id=doc_id, body={"doc": filename_data})async def process_file(file_path: str, doc_id: str):    try:        filename_data = extract_data(file_path)        update_status(doc_id, filename_data)    finally:        os.remove(file_path)@app.post("/upload-pdf/")async def upload_pdf(background_tasks: BackgroundTasks, file: UploadFile = File(...)):    file_location = os.path.join(DATA_PDFS_DIR, file.filename)    with open(file_location, "wb") as f:        f.write(await file.read())    doc_id = store_initial_status()    background_tasks.add_task(process_file, file_location, doc_id)    return {"status": "processing", "id": doc_id}@app.post("/upload-multiple-pdfs/")async def upload_multiple_pdfs(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...)):    doc_ids = []    for file in files:        file_location = os.path.join(DATA_PDFS_DIR, file.filename)        with open(file_location, "wb") as f:            f.write(file.file.read())                doc_id = store_initial_status()        background_tasks.add_task(process_file, file_location, doc_id)        doc_ids.append(doc_id)        return {"status": "processing", "ids": doc_ids}@app.post("/process-directory/")async def process_directory(background_tasks: BackgroundTasks, directory: str):    if not os.path.isdir(directory):        raise HTTPException(status_code=400, detail="Invalid directory path")        doc_ids = []    for filename in os.listdir(directory):        if filename.lower().endswith('.pdf'):            file_path = os.path.join(directory, filename)            file_location = os.path.join(DATA_PDFS_DIR, filename)            os.rename(file_path, file_location)            doc_id = store_initial_status()            background_tasks.add_task(process_file, file_location, doc_id)            doc_ids.append(doc_id)        return {"status": "processing", "ids": doc_ids}@app.get("/status/")async def get_all_statuses():    try:        response = es.search(index="pdf_data", body={"query": {"match_all": {}}})        statuses = [{"id": hit["_id"], "status": hit["_source"]["status"]} for hit in response["hits"]["hits"]]        return {"statuses": statuses}    except Exception as e:        raise HTTPException(status_code=500, detail=str(e))@app.get("/status/{doc_id}")async def get_status_by_id(doc_id: str):    try:        response = es.get(index="pdf_data", id=doc_id)        return {"id": response["_id"], "status": response["_source"]["status"]}    except Exception as e:        raise HTTPException(status_code=500, detail=str(e))@app.get("/status-by-filename/")async def get_status_by_filename(filename: str = Query(..., description="The filename to search for")):    try:        response = es.search(index="pdf_data", body={            "query": {                "match": {                    "filename": filename                }            }        })        statuses = [{"id": hit["_id"], "status": hit["_source"]["status"} for hit in response["hits"]["hits"]]        return {"statuses": statuses}    except Exception as e:        raise HTTPException(status_code=500, detail=str(e))```### 3. requirements.txtList of Python dependencies.```fastapiuvicornelasticsearch```### 4. start_elasticsearch_kibana.shA shell script to start Elasticsearch and Kibana using Docker.```#!/bin/bash# Start Elasticsearchdocker run -d --name elasticsearch -p 9200:9200 -e "discovery.type=single-node" docker.elastic.co/elasticsearch/elasticsearch:7.17.0# Start Kibanadocker run -d --name kibana -p 5601:5601 --link elasticsearch:elasticsearch docker.elastic.co/kibana/kibana:7.17.0```## InstructionsSet Up Elasticsearch and KibanaRun the following script to start Elasticsearch and Kibana using Docker:```chmod +x start_elasticsearch_kibana.sh./start_elasticsearch_kibana.sh```## Install Python DependenciesEnsure you have Python installed, then install the dependencies using pip:```pip install -r requirements.txt```## Run the FastAPI ApplicationStart the FastAPI application using uvicorn:```uvicorn main:app --reload```## Interact with the APIYou can now upload PDF files and check their statuses via the API endpoints. The uploaded PDFs will be saved in the data_pdfs directory. Use tools like curl or Postman to test the endpoints.Example curl CommandsTo test the upload-pdf endpoint:```curl -X POST "http://127.0.0.1:8000/upload-pdf/" -F "file=@path/to/your/file.pdf"```To check the status of all documents:```curl -X GET "http://127.0.0.1:8000/status/"```To check the status of a specific document by its ID:```curl -X GET "http://127.0.0.1:8000/status/{doc_id}"```To check the status of a document by its filename:```curl -X GET "http://127.0.0.1:8000/status-by-filename/?filename=your_file.pdf"```By following these steps, you'll have Elasticsearch, Kibana, and the FastAPI application running and ready to handle PDF uploads and store their statuses in Elasticsearch.